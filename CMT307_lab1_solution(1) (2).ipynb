{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzbr88wr1umw"
   },
   "source": [
    "## CMT307 Lab 1 Exercise\n",
    "Download Credit Approval Data Set from UCI Machine Learning Repsoitory. Do:\n",
    "\n",
    "- practicing exploratory data analysis\n",
    "- dealing with missing values if any\n",
    "- encoding categorical features\n",
    "- scaling features\n",
    "- if you have time, implementing a classifier to predict if a credit card application is approved (+ of the last column) or reject (- of the last column)\n",
    "\n",
    "You can read more information about the data set from https://archive.ics.uci.edu/ml/datasets/Credit+Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sYNurIkY17b0"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-55018f101d7c>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-55018f101d7c>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    Get Credit Approval Data Set\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Get Credit Approval Data Set\n",
    "crx = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data', header='infer')\n",
    "crx.columns = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'Target']\n",
    "\n",
    "# Start writing your IPython notebook............\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eX9FjE4okOAw"
   },
   "source": [
    "## Sample solution\n",
    "This sample solution is only an illustration of an implementation of the tasks in the exercise. It doesn't mean it is the rightest or best soulution, nor it is optimised for best classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TdigpHClLmE"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None, 'max_colwidth', None, 'display.expand_frame_repr', False) # print all columns in full, prevent line break\n",
    "\n",
    "print(crx.shape)\n",
    "print(crx.head())\n",
    "print(crx.info())\n",
    "print(crx.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji0nou_5yDdf"
   },
   "source": [
    "It is clear that A2 and A14 are neumeric attributes from crx.head(), but they are showing 'object' type from crx.info(), and they aren't included in crx.describe(). This indicates A2 and A14 contain other date type, e.g., strings to represent missing value. You can see A2 contains question marks '?' for missing values if you print it out using the following code. You may also inspect if the dataset contains missing values in Excel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9y3pN6k0ACP"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(crx[['A2', 'A14']])\n",
    "#print('\\nA2\\n', crx['A2'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwHdy5pfGHog"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "crx.replace(to_replace='?', value=np.nan, inplace=True)\n",
    "crx[['A2', 'A14']] = crx[['A2', 'A14']].astype(float)\n",
    "\n",
    "crx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lpq0APvS67tJ"
   },
   "outputs": [],
   "source": [
    "crx_x = crx[['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15']]\n",
    "crx_y = crx[['Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PNA_4F5b7i0M"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(crx_x, crx_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "14BQ9zD68MKg"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.compose import make_column_transformer\n",
    "\n",
    "# transformer for categorical features\n",
    "categorical_features = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13', ]\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        ('imputer_cat', SimpleImputer(strategy = 'most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# transformer for numerical features\n",
    "numeric_features = ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']\n",
    "numeric_transformer = Pipeline(\n",
    "    [\n",
    "        ('imputer_num', SimpleImputer(strategy = 'median')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# combine them in a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('categoricals', categorical_transformer, categorical_features),\n",
    "        ('numericals', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder = 'drop'\n",
    ")\n",
    "\n",
    "\n",
    "#crx_processed = preprocessor.fit_transform(crx_x)\n",
    "\n",
    "#np.set_printoptions(threshold=np.inf, linewidth=np.inf, suppress=True, precision=2)\n",
    "#print(crx_processed[0:10, :])\n",
    "#crx_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GbtYW_hHHxuF"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "myClassfier = Pipeline(\n",
    "    [\n",
    "     ('preprocessing', preprocessor),\n",
    "     ('classifier', KNeighborsClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8op4lx_H4AP"
   },
   "outputs": [],
   "source": [
    "myClassfier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLTdvfniH5Il"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = myClassfier.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CMT307_lab1_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
