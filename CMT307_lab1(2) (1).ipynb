{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CmjxlpGkItr"
   },
   "source": [
    "# Practical Session 1 \n",
    "This session consists of some first steps of a machine learning process:\n",
    "\n",
    "*   loading data\n",
    "*   data exploration\n",
    "*   data pre-processing including\n",
    "    * dealing with missing values\n",
    "    * encoding categorical features\n",
    "    * feature scaling\n",
    "\n",
    "Author: Yuhua Li\n",
    "\n",
    "Date:   November 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gJ-6oXKtkBok"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0597a4ad7e3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTsOIpKnb2cB"
   },
   "source": [
    "# Get data\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZlAAVesExbA0"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-df5ae32069d3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-df5ae32069d3>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    load Titanic data from URL\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "load Titanic data from URL\n",
    "titanic = pd.read_csv('http://bit.ly/kaggletrain')\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/pandas-videos/master/data/titanic_train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE1cR08HckyQ"
   },
   "source": [
    "Have an initia inspection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73Eo49s0xgyX"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None, 'max_colwidth', None, 'display.expand_frame_repr', False) # print all columns in full, prevent line break\n",
    "\n",
    "print('\\nInformation of the titanic dataset...... \\n')\n",
    "print(titanic.info())\n",
    "print('\\nColumn names of the dataset...... \\n', titanic.columns)\n",
    "\n",
    "\n",
    "print('\\nBelow is the first few lines of the dataset......\\n', titanic.head(10))\n",
    "print('\\nBelow is the statistics of the dataset......\\n', titanic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LrB3DENzT3O"
   },
   "source": [
    "The dataset contains columns for passenger IDs, names and ticket numbers, which aren't useful for modelling. So we can remove 'PassengerId', 'Name', 'Ticket' columns from the original dataset. Note  generally we need to remove personally identifiable information from a dataset to avoid violation of the law of GDPR (Genrla Data Protection Regulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tatxNI_R0eR3"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  titanic.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)\n",
    "except KeyError:\n",
    "  print('Attributes already removed')\n",
    "\n",
    "print(titanic.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5kJgvoSfwz7"
   },
   "source": [
    "# Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iUFXRg1gFtR"
   },
   "source": [
    "We first check if there are missing values in each feature, 0 for no missing value or the number for the number of missing values. You will see there missing values for Age, Cabin and Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOcc_sjmtwbY"
   },
   "outputs": [],
   "source": [
    "titanic.isna().sum()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQu6EUA0hc1R"
   },
   "source": [
    "As you can see from above, there are missing values in this Titanic dataset. \n",
    "\n",
    "Some packages of machine learning methods provide the ability of dealing with a dataset with missing values so they can take the data directly without us explicitly dealing with missing values. However, many others need us to process missing values before feeding the dataset for machine leanring modelling.\n",
    "\n",
    "You have a few options to deal with missing values:\n",
    "1. Get rid of the corresponding instances.\n",
    "2. Get rid of the whole attribute.\n",
    "3. Set the values to some value (zero, the mean, the median, etc.).\n",
    "4. Use imputation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjd9PhFdn9Zx"
   },
   "source": [
    "## 1. Get rid of the corresponding instances containing missing values. \n",
    "If the number of instances in a dataset is large and the fraction of intances with missing values is samll, an easy way is simply to remove those instances containing missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYluqRpO4RZW"
   },
   "outputs": [],
   "source": [
    "print('Data size BEFORE deleteting instances with missing values: ', titanic.shape)\n",
    "\n",
    "titanic_ins = titanic.dropna(subset=['Age', 'Cabin', 'Embarked'])\n",
    "print('\\nData size AFTER deleteting instances containing missing values: ', titanic_ins.shape)\n",
    "titanic_ins.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g713KJ878VCo"
   },
   "source": [
    "## 2. Get rid of the whole attribute.\n",
    "\n",
    "As seen above, the feature Cabin contains 687 missing values which is a significant portion of the total instances (891). If we remove those instances based on Cabin feature (and Age and Embarked), we only get 183 instances left from the original 891 instances, which means we lost majority portion of the original dataset. So we'd better drop the feature of *Cabin* entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUsR0ZhzAh__"
   },
   "outputs": [],
   "source": [
    "titanic1 = titanic.drop(\"Cabin\", axis=1)\n",
    "print(titanic1.head())\n",
    "titanic1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taweZjZhEzbD"
   },
   "source": [
    "## 3. Set the missing values to some value\n",
    "After removing the entire feature of *Cabin*, the resulting dataset ***titanic1*** still contains missing values for *Age* and *Embarked*. We may fill the missing values by statistics, e.g., mean and median of a numeric feature or the most frequent value of a categorical feature, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3rYmdwtGvT2"
   },
   "outputs": [],
   "source": [
    "# Age is a numeric feature, we may replace missign values by the median of Age \n",
    "median = titanic1[\"Age\"].median() # option 3\n",
    "titanic1[\"Age\"].fillna(median, inplace=True)\n",
    "print('After filling missing values of Age\\n', titanic1.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVB8AQc5XHv2"
   },
   "source": [
    "## 4. Use imputation methods.\n",
    "sci-kit learn provide many imputation methods. Here we replace missing values of *Embarked* using its mode, i.e., its most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2a5CfDTXzdd"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "imputer.fit(titanic1)\n",
    "embarked = imputer.transform(titanic1)\n",
    "\n",
    "#print(embarked[:20, ])\n",
    "print(embarked.shape)\n",
    "print(pd.DataFrame(embarked).isna().sum())\n",
    "pd.DataFrame(embarked).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQkdN_-8i_db"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGaBY4GXjFKr"
   },
   "source": [
    "## Scatter plot\n",
    "\n",
    "Scatter plot shows data distribution of a pair of features, it can visuallly reveal the relationship between a feature pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHSPYTLnYxYx"
   },
   "outputs": [],
   "source": [
    "# use scatter_matrix of pandas.plotting\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(titanic1[['Age', 'SibSp', 'Parch', 'Fare']], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61YdvhXskF9U"
   },
   "outputs": [],
   "source": [
    "# use plot of pandas DataFrame\n",
    "titanic1.plot(kind='scatter', x='Age', y='Fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_JUUgqF-X-g"
   },
   "source": [
    "## Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6193lUo-cB9"
   },
   "outputs": [],
   "source": [
    "print(titanic1[['Age', 'Fare']].describe())\n",
    "titanic1[['Age', 'Fare']].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fDUyBxFlh1p"
   },
   "source": [
    "## Correlation\n",
    "Correlation coefficient measures linear relationship between a pair of variables, range [-1, 1]. 1 indicates full linear relationship, 0 no linear relationship, -1 negatively linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHBrlYX7lok1"
   },
   "outputs": [],
   "source": [
    "corr_matrix = titanic1.corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phhIiHvmmAlJ"
   },
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30YAnXC5mE-4"
   },
   "outputs": [],
   "source": [
    "titanic1['Age'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nqu0cCEBr4MX"
   },
   "source": [
    "# Feature scaling\n",
    "Feature scaling is a neccessary steps for most machine learning methods in order to achieve good learning performance and faster learning process.\n",
    "\n",
    "There are two commonly used feature scaling methods: \n",
    "1. Scaling features to a range: e.g., min-max scaling\n",
    "2. Standardization: zero mean and unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARP3fjyYyjGh"
   },
   "source": [
    "## 1. Scaling features to a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJJtkd1qAf-5"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minMax_scale = MinMaxScaler()   # to default range [0, 1]\n",
    "titanic1['Age'] = minMax_scale.fit_transform(titanic1[['Age']])\n",
    "\n",
    "titanic1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCK6BDMpD9k1"
   },
   "source": [
    "## Standardization (z-score)\n",
    "Standardization scales a featue to a feature with 0 mean and 1 standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3ptzvvEDTZq"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scale = StandardScaler()   # to default 0 mean and 1 standard deviation\n",
    "titanic1['Fare'] = standard_scale.fit_transform(titanic1[['Fare']])\n",
    "\n",
    "titanic1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO9pSJCU8HzF"
   },
   "source": [
    "# Encoding categorical features\n",
    "\n",
    "Most machine learning methods take only numerical data, except decision tree based methods which can take numerical and categorical features directly. So we need to convert categories to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgqdRLL48VuK"
   },
   "source": [
    "## One hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSwKtvOr8avC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "titanic1 = titanic[['Pclass', 'Sex', 'Embarked']].dropna()\n",
    "print(titanic1.info())\n",
    "print('\\n', titanic1.head())\n",
    "\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "enc.fit(titanic1)\n",
    "\n",
    "print('\\n', enc.categories_)\n",
    "titanic2 = pd.DataFrame(enc.transform(titanic1))\n",
    "titanic2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRCO8oBbDslK"
   },
   "source": [
    "## Effect encoding\n",
    "A feature with N categories will produce N binary features after one hot encoding as above. One of the N binary features is perfectly collinear with the other N-1 features. This means there are only N-1 non-collinear binary features for a N-category feature. So we can drop one of the derived binary features as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vkSzbuIDhPq"
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(drop='first', sparse=False)\n",
    "titanic1 = titanic[['Pclass', 'Sex', 'Embarked']].dropna()\n",
    "print(titanic1.head())\n",
    "enc.fit(titanic1)\n",
    "titanic2 = pd.DataFrame(enc.transform(titanic1))\n",
    "print(titanic2.shape)\n",
    "\n",
    "titanic2 = pd.concat([titanic2, titanic[['Age', 'SibSp',  'Parch', 'Fare']]], axis=1)\n",
    "titanic2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8lzZlWUfYhq"
   },
   "source": [
    "## Use Transformer\n",
    "Or use make_column_transformer to encode the named categorical features and keep other features unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsSL3IEugLKx"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "imputer.fit(pd.DataFrame(titanic1[\"Embarked\"]))\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "titanic3 = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "print('Before filling missing values of Age\\n', titanic3.isna().sum())\n",
    "print(titanic3.shape)\n",
    "print(titanic3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JcWENd6NagB"
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "titanic3[\"Embarked\"] = imputer.fit_transform(pd.DataFrame(titanic3[\"Embarked\"]))\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "titanic3[\"Age\"] = imputer.fit_transform(pd.DataFrame(titanic3[\"Age\"]))\n",
    "print('After filling missing values of Age\\n', titanic3.isna().sum())\n",
    "\n",
    "print(titanic3.shape)\n",
    "print(titanic3)\n",
    "\n",
    "col_trans = make_column_transformer(\n",
    "    #(SimpleImputer(strategy=\"most_frequent\", add_indicator=True), [\"Embarked\"]),\n",
    "    #(SimpleImputer(strategy=\"median\", add_indicator=True), [\"Age\"]),\n",
    "    (OneHotEncoder(), ['Sex', 'Embarked']),\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "titanic_ce = col_trans.fit_transform(titanic3)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf, suppress=True, precision=2)\n",
    "print(titanic_ce[0:20, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W50ActG5-2me"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Read Category Encoders for other encoding methods on https://contrib.scikit-learn.org/category_encoders/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW2m5iwMevSa"
   },
   "source": [
    "# First machine learning project\n",
    "Finally we put things together to practice our first machine learning project, here we build a k-nerest neighbours model to predict Suvival based on passengers' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tkmGLJBfHxL"
   },
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISWF_tuMHWEK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Get a new copy of titanic data. \n",
    "# We ignore 'PassengerId', 'Name' and 'Ticket', as they are useless for machine learnign.\n",
    "# We also ignore 'Cabin' as it contains too many missing values.\n",
    "titanic = pd.read_csv('http://bit.ly/kaggletrain')\n",
    "titanic.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)\n",
    "\n",
    "pd.set_option('display.max_columns', None, 'max_colwidth', None, 'display.expand_frame_repr', False)\n",
    "print(titanic)\n",
    "\n",
    "titanic_x = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "titanic_y = titanic[['Survived']]\n",
    "\n",
    "print(titanic_x.head(20))\n",
    "#print(titanic_x['Embarked'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BuO71WdfTlG"
   },
   "source": [
    "## Split data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEQcalcmfbnn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_x, titanic_y, test_size=0.3, random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8luxwUYG_o7"
   },
   "source": [
    "## Preprocessing Pipeline & ColumnTransformer\n",
    "We now put all transformations using Pipeline & ColumnTransformer. Doing so not only make code tidy but also have advantages:\n",
    "\n",
    "*   allow to include the preprocessing steps in the hyperparameter tuning (will learn later)\n",
    "*   avoid data leakage, i.e., avoid making the mistake of using any test data for model training\n",
    "*   guarantee that your data is always preprocessed the same way. For example,  if a categorical feature has a category in the test set that does not occur in the training set or a category in the training set that doesn't occur in the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nl0M9DOcJVId"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.compose import make_column_transformer\n",
    "\n",
    "# transformer for categorical features\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        ('imputer_cat', SimpleImputer(strategy = 'most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# transformer for numerical features\n",
    "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "numeric_transformer = Pipeline(\n",
    "    [\n",
    "        ('imputer_num', SimpleImputer(strategy = 'median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# combine them in a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('categoricals', categorical_transformer, categorical_features),\n",
    "        ('numericals', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder = 'drop'\n",
    ")\n",
    "\n",
    "\n",
    "titanic_processed = preprocessor.fit_transform(titanic_x)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf, suppress=True, precision=2)\n",
    "\n",
    "print(titanic_processed[0:20, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybRi-22yp3sv"
   },
   "source": [
    "## Define the model\n",
    "We use a pipeline to put together the preprocessor from above and the k-nearest neighbours classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZqn1-AoqSyG"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "myClassfier = Pipeline(\n",
    "    [\n",
    "     ('preprocessing', preprocessor),\n",
    "     ('classifier', KNeighborsClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zth4PnClryoI"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SO4C-Unr2Pg"
   },
   "outputs": [],
   "source": [
    "\n",
    "myClassfier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9dbx26Or7gk"
   },
   "source": [
    "## Evaluate the model\n",
    "We evalute the performance of the trained classifier on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRJ4jFOCr_BU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = myClassfier.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzbr88wr1umw"
   },
   "source": [
    "# Exercise\n",
    "Download Credit Approval Data Set from UCI Machine Learning Repsoitory. Do:\n",
    "\n",
    "- practicing exploratory data analysis\n",
    "- dealing with missing values if any\n",
    "- encoding categorical features\n",
    "- scaling features\n",
    "- if you have time, implementing a classifier to predict if a credit card application is approved (+ of the last column) or reject (- of the last column)\n",
    "\n",
    "You can read more information about the data set from https://archive.ics.uci.edu/ml/datasets/Credit+Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYNurIkY17b0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get Credit Approval Data Set\n",
    "crx = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data', header='infer')\n",
    "crx.columns = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'Target']\n",
    "\n",
    "# Start writing your IPython notebook............\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CMT307_lab1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
